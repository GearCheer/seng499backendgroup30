"""
Written in Python 3.7.3

Requires keras and tensorflow

Install dependencies using:
    pip3 install keras
    pip3 install tensorflow
"""

from keras.applications.resnet50 import ResNet50
from keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator
from keras.layers import Dense, GlobalAveragePooling2D
from keras.models import Model, load_model
import tensorflow as tf
from sklearn.metrics import confusion_matrix
import numpy as np
import time
import cv2

# CONSTANTS
MODEL_PATH = "models/model.h5"
NUM_CLASSES = 9 # Number of classes (1 for each folder in dataset).

DEBUG_RECORD_TIMES = True # When true, records classification times. Only use for brief testing, as memory usage is unbound

# For full dataset
DATASET_PATH = "reclassified-dataset" # Path to dataset from working directory.
NUM_EPOCHS = 80
BATCH_SIZE = 32  # Number of images in each batch generated by the image generator.
                 # Equals the number of images considered before updating weights when training the model.
VALIDATION_STEPS = 10 # number of testing images divided by the batch size

# For small testing dataset
# DATASET_PATH = "reclassified-dataset-small" # Path to dataset from working directory.
# NUM_EPOCHS = 18
# BATCH_SIZE = 5
# VALIDATION_STEPS = 17

class Classifier:

    def __init__(self, model_path=None, imageset_path=None):

        self.model = None
        self.graph = tf.get_default_graph()

        # Used for reporting average classification time
        self.prediction_times = []
        self.n_predictions = 0

        if imageset_path:
            self._train_new_model(imageset_path)
        elif model_path:
            self._load_model_weights(model_path)

    def _train_new_model(self, imageset_path):

        datagen = ImageDataGenerator()

        # Create iterator for training images
        train_it = datagen.flow_from_directory(imageset_path + '/Train', target_size=(384, 512), batch_size=BATCH_SIZE)
        test_it  = datagen.flow_from_directory(imageset_path + '/Test', target_size=(384, 512), batch_size=BATCH_SIZE)


        # Create model with ResNet50 weights, but drop top layer
        res_model = ResNet50(input_shape=(384, 512, 3), weights='imagenet', include_top=False)

        # Add new top layer.  This is a fully connected layer with one node for each output class.
        x = res_model.output
        x = GlobalAveragePooling2D()(x)

        x = Dense(4096, activation='relu')(x) # Optional extra Dense layer.  Seeing if this improves accuracy

        predictions = Dense(NUM_CLASSES, activation='softmax')(x)
        self.model = Model(inputs=res_model.input, outputs=predictions)

        # Set all but last layer to be untrainable
        for layer in self.model.layers[:-26]:
            layer.trainable=False

        # Set last layer to be trainable
        for layer in self.model.layers[-26:]:
            layer.trainable=True

        # Compile and fit
        self.model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy']) # old: categorical_crossentropy
        self.model.fit_generator(train_it, epochs=NUM_EPOCHS, steps_per_epoch=BATCH_SIZE, verbose=1, validation_data=test_it, validation_steps=VALIDATION_STEPS)

    def _load_model_weights(self, model_path):
        self.model = load_model(model_path)

    def save_model_weights(self):
        self.model.save('models/test_model.h5')

    # Outputs an array of floats from 0 to 1
    def classify(self, img):
        
        with self.graph.as_default():
            if DEBUG_RECORD_TIMES:
                start_time = time.time()

            result = self.model.predict(img)

            if DEBUG_RECORD_TIMES:
                elapsed_time = time.time() - start_time
                self.record_classification_time(elapsed_time)
            return result

    def record_classification_time(self, time):
        if DEBUG_RECORD_TIMES:
            self.n_predictions += 1
            self.prediction_times.append(time)
            print("Prediction Time: " + str(time), "seconds -- Average: " + str(sum(self.prediction_times)/self.n_predictions) + " seconds -- Number of Predictions: " + str(self.n_predictions))

    def confusion_matrix(self):
        datagen = ImageDataGenerator()
        test_generator = datagen.flow_from_directory(DATASET_PATH + '/Test', target_size=(384, 512), batch_size=BATCH_SIZE)
        Y_pred = self.model.predict_generator(test_generator, VALIDATION_STEPS)
        y_pred = np.argmax(Y_pred, axis=1)
        print(confusion_matrix(test_generator.classes, y_pred))

# executes testing code when run as main
if __name__ == "__main__":
    # Generate and save a model
    # wc = Classifier(imageset_path=DATASET_PATH)
    # wc.save_model_weights()

    # Load a model
    wc2 = Classifier(model_path=MODEL_PATH)

    # Confusion Matrix
    # wc2.confusion_matrix()

    # Load image and classify it


    img1 = cv2.imread('backup-images/Compost/IMG_20190718_123259873.jpg')
    img1 = cv2.resize(img1, (384, 512))
    img1 = np.reshape(img1, [1, 384, 512, 3])
    wc2.classify(img1)

    img2 = cv2.imread('backup-images/Compost/IMG_20190718_123301801.jpg')
    img2 = cv2.resize(img2, (384, 512))
    img2 = np.reshape(img2, [1, 384, 512, 3])
    wc2.classify(img2)


